{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b34c8cd8",
   "metadata": {},
   "source": [
    "# Pulling Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "052022dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbf438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sending GET requests from the API\n",
    "import requests\n",
    "\n",
    "# For saving access tokens and for file management when creating and adding to the dataset\n",
    "import os\n",
    "\n",
    "# For dealing with json responses received from the API\n",
    "import json\n",
    "\n",
    "# For displaying and managing the data\n",
    "import pandas as pd\n",
    "\n",
    "# For saving the data into a CSV format\n",
    "import csv\n",
    "\n",
    "# For parsing the dates received from Twitter into readable formats\n",
    "import datetime as dt\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "\n",
    "#To add wait time between requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8208ad09",
   "metadata": {},
   "source": [
    "## Authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bc83ed",
   "metadata": {},
   "source": [
    "Autheticate via `local-api-key.txt`, which is just a file in the root directory of my project that is ignored by github (see the `.gitignore` file).  The contents of this file are just:\n",
    "\n",
    "```\n",
    "API_Key: <text of the key>\n",
    "API_Secret: <text of the secret>\n",
    "Bearer: <text of the bearer>\n",
    "```\n",
    "It turns out that all we need is the Bearer text, so the other two lines are really optional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96b87c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "creds = {}\n",
    "with open(\"../../../local-api-key.txt\") as f:\n",
    "    for line in f:\n",
    "        pieces = line.split(\":\")\n",
    "        #print(pieces)\n",
    "        creds[pieces[0].strip()]=pieces[1].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdbd535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication method for the v2 Twitter API\n",
    "client = tweepy.Client(creds['Bearer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b98866",
   "metadata": {},
   "source": [
    "### Media Account Names and IDs\n",
    "\n",
    "Create a dictionary of all accounts with twitter ids here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87ba10c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_ids = {\"crooksandliars\":14513611}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa48f16",
   "metadata": {},
   "source": [
    "## Identifying Candidate Followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad96e9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the follower results incrementally to a file as they are retrieved \n",
    "def write_results(account,candidates):\n",
    "    file = f\"./{account}.csv\"\n",
    "    new = not os.path.exists(file)\n",
    "    with open(file,\"a\") as f:\n",
    "        out = csv.DictWriter(f,[\"id\",\"tweet_count\",\"created_at\",\"name\",\"username\",\"location\"])\n",
    "        if new:\n",
    "            out.writeheader()\n",
    "        out.writerows(candidates)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The following loop runs over the \"handle_ids\" dictionary and retrieves followers\n",
    "# for each account, and then saves them to a file.\n",
    "\n",
    "# I'm picking people that have been on the site since 2012,1,1 and have at least 500\n",
    "# posts.  Ultimately, I'm going to screen out people with too many tweets as well - we\n",
    "# can fill those in later\n",
    "\n",
    "# Note that this function will give us more than min_candidates (200) followers.  Doing this\n",
    "# in the hopes we'll find a subset of users that work for the study (i.e. relatively even \n",
    "# participation over the decade)\n",
    "\n",
    "start = dt.datetime(2022,1,1)\n",
    "min_desired_posts = 500\n",
    "max_desired_posts = 50000\n",
    "desired_tenure = 10\n",
    "min_candidates = 200\n",
    "\n",
    "for account,id in handle_ids.items():\n",
    "    done = False\n",
    "    token = None\n",
    "    candidate_count = 0\n",
    "    call_count = 2\n",
    "    \n",
    "    while candidate_count < min_candidates and not done:\n",
    "        candidates = []\n",
    "        result = client.get_users_followers(id,expansions=None,max_results=1000,pagination_token=token,\n",
    "            user_fields = [\"created_at\",\"protected\",\"public_metrics\",\"location\"])\n",
    "        \n",
    "        call_count+=1\n",
    "        #print(f\"Results meta: {result.meta}\")\n",
    "        if len(result.errors) > 0 or call_count % 15 == 0:\n",
    "            #presume rate limit, sleep for 15 min (ugh)\n",
    "            call_count = 0\n",
    "            print(\"Sleep for 15 minutes...\")\n",
    "            time.sleep(60 * 15)\n",
    "        else:\n",
    "            token = result.meta[\"next_token\"]\n",
    "            if not token:\n",
    "                done = True\n",
    "            for u in result.data:\n",
    "                if not u.protected:\n",
    "                    data = {}\n",
    "                    data['tweet_count'] = u.public_metrics['tweet_count']\n",
    "                    data['created_at'] = u.created_at.replace(tzinfo=None)\n",
    "                    if data['tweet_count'] >= min_desired_posts and data['tweet_count'] <= max_desired_posts and (start-data['created_at']).days/365 >= desired_tenure:\n",
    "                        data['id'] = u.id\n",
    "                        data['name'] = u.name\n",
    "                        data['username'] = u.username\n",
    "                        data['location'] = u.location\n",
    "                        candidates.append(data)\n",
    "            candidate_count += len(candidates)        \n",
    "            print(f\"Call {call_count} added {len(candidates)} for {candidate_count} total\")\n",
    "            write_results(account,candidates)\n",
    "           \n",
    "                     \n",
    "                \n",
    "            \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f8d27f",
   "metadata": {},
   "source": [
    "# Getting the user's history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1a18d485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the response from Twitter (which is split into a main response and an \"includes\" portion)\n",
    "# and then merges them together.  It is expecting json as attributes (data, includes) and returns a single\n",
    "# merged json document.  You don't need to call this, as it is being called by other routines below\n",
    "# \n",
    "# Note that this is *destructive* (it modifies the original object) so we'll want to be careful.\n",
    "# Also note, though I'm printing out a warning when there is no data, the code will still\n",
    "# error out when this happens.  I'm still not sure what's going on, but it doesn't seem to occur\n",
    "# frequently enough for it to matter.  I will return to this later on, or maybe you can investigate.\n",
    "# Just keep track of these errors (see below).\n",
    "\n",
    "def merge_data(data,includes):\n",
    "    if not data:\n",
    "        print(\"Warning: no data to merge!!!\")\n",
    "\n",
    "    if not includes:\n",
    "        return data\n",
    "    user_map = {u.id:u.data for u in includes['users']} if 'users' in includes else None\n",
    "    tweet_map = {t.id:t.data for t in includes['tweets']} if 'tweets' in includes else None\n",
    "    #print(tweet_map)\n",
    "    for tweet in data:\n",
    "        tweet_obj = tweet.data\n",
    "        if 'referenced_tweets' in tweet_obj and tweet_map:\n",
    "            full_text = None\n",
    "            for idx,rt in enumerate(tweet['referenced_tweets']):\n",
    "                rt_id = int(rt['id'])\n",
    "                if int(rt_id) in tweet_map:\n",
    "                    tweet_obj['referenced_tweets'][idx]['expanded'] = tweet_map[rt_id]\n",
    "                    if rt['type']==\"retweeted\":\n",
    "                        full_text = tweet_map[rt_id]['text']\n",
    "                \n",
    "                    \n",
    "            if full_text:\n",
    "                rt_part = tweet_obj['text'].split(\": \")[0]\n",
    "                tweet_obj['text'] = f\"{rt_part}: {full_text}\"\n",
    "        if user_map and 'entities' in tweet_obj and 'mentions' in tweet_obj['entities']:\n",
    "            for idx, user in enumerate(tweet['entities']['mentions']):\n",
    "                user_id = int(user['id'])\n",
    "                if user_id in user_map:\n",
    "                    tweet_obj['entities']['mentions'][idx]['expanded'] = user_map[user_id]\n",
    "\n",
    "\n",
    "    return(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b783ae",
   "metadata": {},
   "source": [
    "The following is just for writing the user's tweets to a json file.  I do a very little bit of editing to make sure we have a properly formatted file.  I like to keep the raw json files around in case we screw up the csv conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ba4640ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_user_data(account,user_id,data,finalize = False):\n",
    "   \n",
    "    file = f\"./{user_id}_{account}.json\"\n",
    "    new = not os.path.exists(file)\n",
    "    with open(file,\"a\") as f:\n",
    "        if new:\n",
    "            f.write(\"[\")\n",
    "        if data:\n",
    "            for idx,tweet in enumerate(data):\n",
    "                f.write(json.dumps(tweet.data))\n",
    "                if idx < len(data) -1:\n",
    "                    f.write(\",\")\n",
    "        if finalize:\n",
    "            f.write(\"]\")\n",
    "        else:\n",
    "            f.write(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d245f1",
   "metadata": {},
   "source": [
    "## Collecting the user data\n",
    "\n",
    "The following is the loop to collect data from the actual users.  It is in fact opening up the users file we created before, and pulling the ids out of that.  You will want to filter that file to make sure we only have english speaking folks before we pull their entire history.  So, your first step is to build that logic.  You can base that on the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5731981f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 284 of 321 : ZachRarick (about 625 posts)\n",
      "..\n",
      "Skipping bebe57roy because they have 60753 tweets\n",
      "Processing 286 of 321 : adacarey (about 7173 posts)\n",
      ".........\n",
      "Processing 287 of 321 : charleydeppner (about 6862 posts)\n",
      ".........\n",
      "Processing 288 of 321 : EricXWest (about 11763 posts)\n",
      ".......\n",
      "Processing 289 of 321 : MadonnaFigura (about 8055 posts)\n",
      ".........\n",
      "Skipping shanasshots because they have 137095 tweets\n",
      "Processing 291 of 321 : CarmeninSD (about 24213 posts)\n",
      "\n",
      "Processing 292 of 321 : Karenb1038 (about 1884 posts)\n",
      "....\n",
      "Processing 293 of 321 : tabbycat79 (about 5067 posts)\n",
      ".........\n",
      "Processing 294 of 321 : unabombershack (about 6392 posts)\n",
      ".........\n",
      "Processing 295 of 321 : katbanner (about 1796 posts)\n",
      "....\n",
      "Processing 296 of 321 : k_macC_ (about 1761 posts)\n",
      "....\n",
      "Processing 297 of 321 : LesleyKSmith (about 15145 posts)\n",
      ".........\n",
      "Processing 298 of 321 : lmlinflorida (about 1120 posts)\n",
      "...\n",
      "Skipping clc202000 because they have 200966 tweets\n",
      "Skipping sbryt because they have 113947 tweets\n",
      "Processing 301 of 321 : Mystic_AL13n (about 41872 posts)\n",
      ".........\n",
      "Processing 302 of 321 : ca_bulldog (about 11529 posts)\n",
      ".........\n",
      "Processing 303 of 321 : HistoryTeachr61 (about 7501 posts)\n",
      "..........\n",
      "Processing 304 of 321 : preiter20 (about 7273 posts)\n",
      ".........\n",
      "Processing 305 of 321 : Arendt_Center (about 12244 posts)\n",
      ".........\n",
      "Processing 306 of 321 : JamesFSmith1 (about 6246 posts)\n",
      ".........\n",
      "Processing 307 of 321 : choppyguillotte (about 5295 posts)\n",
      ".........\n",
      "Processing 308 of 321 : lilbigguy24 (about 9657 posts)\n",
      "..........\n",
      "Processing 309 of 321 : ajmomauntieann (about 3512 posts)\n",
      "........\n",
      "Processing 310 of 321 : TimeBraid (about 542 posts)\n",
      "..\n",
      "Processing 311 of 321 : Rocknole (about 1868 posts)\n",
      "....\n",
      "Processing 312 of 321 : Alan_Purdy (about 5977 posts)\n",
      ".........\n",
      "Processing 313 of 321 : Bsconley (about 34757 posts)\n",
      ".........\n",
      "Processing 314 of 321 : TimMushel (about 5290 posts)\n",
      ".........\n",
      "Processing 315 of 321 : deborahhy (about 2798 posts)\n",
      "......\n",
      "Processing 316 of 321 : sandismithusa (about 21140 posts)\n",
      ".........\n",
      "Processing 317 of 321 : elementalmama (about 5538 posts)\n",
      ".........\n",
      "Processing 318 of 321 : cflasche (about 4826 posts)\n",
      "........\n",
      "Processing 319 of 321 : oracle410 (about 1258 posts)\n",
      "...\n",
      "Processing 320 of 321 : zgwarnki (about 15955 posts)\n",
      ".........\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "backoff = 10\n",
    "start = dt.datetime.fromisoformat(\"2006-03-21 00:00:00\") #earliest possible tweet\n",
    "\n",
    "# These are referencing the indices of the user file; useful to control things if we error out\n",
    "# and I like to work in batches anyway just to keep an eye on things.  If I were more confident everything\n",
    "# was working, I'd just let it go.  Perhaps as you grow more confident, you will choose to do that. \n",
    "start_idx = 0\n",
    "end_idx = 50\n",
    "\n",
    "# This is the 50k cutoff I chose above\n",
    "max_tweets = 50000\n",
    "\n",
    "for account,id in handle_ids.items():\n",
    "    done = False\n",
    "    token = None\n",
    "    candidate_count = 0\n",
    "    call_count = 2\n",
    "    total_tweets = 0\n",
    "    users_file =  f\"./{account}.csv\"\n",
    "    with open(users_file) as fin:\n",
    "        reader = csv.DictReader(fin)\n",
    "        users = [u for u in reader]\n",
    "\n",
    "    # Note that we could do a lot more here to check and see which user we left off with (in case something\n",
    "    # went wrong) but for now we're just going to hope that things run smoothly enough that we can manage \n",
    "    # it by hand using the start and end index\n",
    "    for i,u in enumerate(users):\n",
    "        if i<start_idx:\n",
    "            continue\n",
    "        if int(u['tweet_count']) > max_tweets:\n",
    "            print(f\"Skipping {u['username']} because they have {u['tweet_count']} tweets\")\n",
    "            continue\n",
    "        print(f\"Processing {i} of {len(users)} : {u['username']} (about {u['tweet_count']} posts)\")\n",
    "        # Twitter API rate limits us at 300 calls per 15 min, which works out to \n",
    "        # 3 seconds per call; so we'll just sleep for that length of time and catch errors\n",
    "        # with a backoff\n",
    "        done = False\n",
    "        user_tweet_count = 0\n",
    "        error_count = 0\n",
    "        #This is just to provide some feedback for (roughly) every 10% of a users tweets recovered\n",
    "        counter_increment = int(u['tweet_count']) / 10\n",
    "        \n",
    "\n",
    "        while not done:\n",
    "            error = False\n",
    "            try:\n",
    "                result = client.search_all_tweets(query=f\"from:{u['id']}\",max_results=500,next_token = token, \n",
    "                    start_time = start,\n",
    "                    place_fields=\"full_name,country_code\",\n",
    "                    expansions = \"referenced_tweets.id,in_reply_to_user_id,entities.mentions.username,referenced_tweets.id.author_id\",\n",
    "                    user_fields = \"description\",\n",
    "                    tweet_fields = \"text,created_at,conversation_id,entities,in_reply_to_user_id,public_metrics,referenced_tweets\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                error = True\n",
    "            if error or (len(result.errors) > 0 and result.meta['result_count'] == 0):\n",
    "                print(f\"{result.errors[0]} and {len(result)-1} others\")\n",
    "                \n",
    "                #presume rate limit, retry three times with incremental backoff\n",
    "                if error_count ==3:\n",
    "                    print(\"Too many errors, bailing\")\n",
    "                    done = True\n",
    "                    break\n",
    "                error_count+=1\n",
    "                print(f\"Sleep for {(backoff ** error_count)/60} minutes\")\n",
    "                time.sleep(backoff ** error_count)\n",
    "            else:\n",
    "                error_count = 0\n",
    "                token = result.meta.get(\"next_token\")\n",
    "                if not token:\n",
    "                    done = True\n",
    "                data = merge_data(result.data,result.includes)\n",
    "                if not data:\n",
    "                    print(f\"No data for {u['id']} : {result.meta})\")\n",
    "                write_user_data(account,u[\"id\"],data, done)\n",
    "                old_count = user_tweet_count\n",
    "                user_tweet_count+=result.meta[\"result_count\"]\n",
    "                if math.floor(user_tweet_count/counter_increment) > math.floor(old_count/counter_increment):\n",
    "                    print(\".\",end=\"\")\n",
    "            time.sleep(3)\n",
    "        print()\n",
    "        if i>end_idx:\n",
    "            print(\"Bailing...\")\n",
    "            break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12c0ae9",
   "metadata": {},
   "source": [
    "Once you are done, go ahead and upload the json files to the relevant directories on the [google drive](https://drive.google.com/drive/u/1/folders/18FX2b3edkKJUZU6IafrY2Dd1xzWprDGR).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c0988",
   "metadata": {},
   "source": [
    "### Noting errors here\n",
    "\n",
    "Keep track of any errors you encounter here, so we can go back and fix things up!\n",
    "\n",
    "1.  User 25258457 - stopped at \"2011-06-17T12:14:21.000Z\" but there are another 500 tweets or so.  Need a special query\n",
    "2.  User 89788214 - Missing the last frame because of an error in merge - last tweet: \"2010-04-18T01:22:01.000Z\".  Need a special query to get these.\n",
    "3.  User 150768615 - stopped at \"2010-08-22T23:26:13.000Z\" - could be the very first tweet for this user?  Unclear how this could happen.\n",
    "4.  User 65109417 - Screwed up and appended the all of these posts again; need to search for the first tweet to repeat and truncate the file\n",
    "5. Unclear error with 182446931 - no data?  Need to return to this one and try again.\n",
    "6. Similarly, no data for 72795851\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('3.9.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "313001d9fa4976b9745bb1b3ab7b30b5e693423a05d0f3ed8ce2fea591a540b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
